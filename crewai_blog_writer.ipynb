{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N2R1_Suff4rx",
      "metadata": {
        "id": "N2R1_Suff4rx"
      },
      "outputs": [],
      "source": [
        "!pip install \"crewai[tools]\" beautifulsoup4 requests transformers ddgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X3g2jkRmzSGE",
      "metadata": {
        "id": "X3g2jkRmzSGE"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "W9-wOqaa8IVJ",
      "metadata": {
        "id": "W9-wOqaa8IVJ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "kYdFHbfzf7Qm",
      "metadata": {
        "id": "kYdFHbfzf7Qm"
      },
      "outputs": [],
      "source": [
        "from crewai import Agent, Task, Crew, Process, LLM\n",
        "from google.colab import userdata\n",
        "\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "gemini_llm = LLM(\n",
        "    model='gemini/gemini-2.5-pro',\n",
        "    api_key=gemini_api_key,\n",
        "    temperature=0.8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "-GjOHvL2veft",
      "metadata": {
        "id": "-GjOHvL2veft"
      },
      "outputs": [],
      "source": [
        "from crewai.tools import BaseTool\n",
        "from typing import Optional\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from ddgs import DDGS\n",
        "\n",
        "class BlogResearchTool(BaseTool):\n",
        "    name: str = \"Blog Research Assistant\"\n",
        "    description: str = \"Retrieves up-to-date information and data from the web for blog content creation\"\n",
        "\n",
        "    def _run(self, topic: str, max_results: int = 5) -> str:\n",
        "        try:\n",
        "            from ddgs import DDGS\n",
        "        except ImportError:\n",
        "            import subprocess\n",
        "            import sys\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"duckduckgo-search\"])\n",
        "            from ddgs import DDGS\n",
        "\n",
        "        results = []\n",
        "        try:\n",
        "            with DDGS() as ddgs:\n",
        "                results = [{\n",
        "                    \"title\": r.get(\"title\", \"\"),\n",
        "                    \"url\": r.get(\"href\", \"\"),\n",
        "                    \"snippet\": r.get(\"body\", \"\"),\n",
        "                    \"source\": urlparse(r.get(\"href\", \"\")).netloc.replace(\"www.\", \"\") if r.get(\"href\") else \"unknown\"\n",
        "                } for r in ddgs.text(topic, max_results=max_results)]\n",
        "        except Exception as e:\n",
        "            return f\"Error during search: {str(e)}\"\n",
        "\n",
        "        data = {\n",
        "            \"trends\": [],\n",
        "            \"stats\": [],\n",
        "            \"devs\": [],\n",
        "            \"opinions\": [],\n",
        "            \"sources\": {r[\"source\"]: r[\"url\"] for r in results if r.get(\"source\")}\n",
        "        }\n",
        "\n",
        "        patterns = {\n",
        "            \"stats\": r'\\b\\d+[\\d,%\\.]+\\s*(?:percent|%|million|billion|years?|months?|\\$)',\n",
        "            \"trends\": r'\\b(?:growth|decline|increase|decrease|trend|adoption|emerging)\\b.*?[\\.\\!\\?]',\n",
        "            \"devs\": r'\\b(?:announce|launch|release|introduce|new|update|version)\\b.*?[\\.\\!\\?]',\n",
        "            \"opinions\": r'[\"“](.*?)[”\"]\\s*—\\s*\\w+\\s*\\w+'\n",
        "        }\n",
        "\n",
        "        for r in results:\n",
        "            if not r.get(\"url\"):\n",
        "                continue\n",
        "\n",
        "            for key, pattern in patterns.items():\n",
        "                matches = re.findall(pattern, r[\"snippet\"], re.IGNORECASE)\n",
        "                for match in matches:\n",
        "                    if match not in data[key]:\n",
        "                        data[key].append(match)\n",
        "\n",
        "            if r[\"source\"] in [\"techcrunch.com\", \"wired.com\", \"mit.edu\"]:\n",
        "                try:\n",
        "                    response = requests.get(r[\"url\"], timeout=10)\n",
        "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                    for tag in [\"script\", \"style\", \"header\", \"footer\", \"nav\"]:\n",
        "                        for element in soup.find_all(tag):\n",
        "                            element.decompose()\n",
        "\n",
        "                    main_content = soup.find('main') or soup.find('article') or soup.body\n",
        "                    text = main_content.get_text(separator=' ', strip=True) if main_content else \"\"\n",
        "                    for key, pattern in patterns.items():\n",
        "                        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "                        for match in matches:\n",
        "                            if match not in data[key]:\n",
        "                                data[key].append(match)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {r['url']}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        summary = f\"Research Summary for '{topic}':\\n\\n\"\n",
        "\n",
        "        summary += \"Key Trends:\\n\"\n",
        "        summary += \"\\n\".join(f\"  - {trend}\" for trend in data[\"trends\"][:5]) or \"  - No trends found\\n\"\n",
        "        summary += \"\\n\\n\"\n",
        "\n",
        "        summary += \"Latest Developments:\\n\"\n",
        "        summary += \"\\n\".join(f\"  - {dev}\" for dev in data[\"devs\"][:5]) or \"  - No developments found\\n\"\n",
        "        summary += \"\\n\\n\"\n",
        "\n",
        "        summary += \"Expert Opinions:\\n\"\n",
        "        summary += \"\\n\".join(f\"  - {opinion}\" for opinion in data[\"opinions\"][:5]) or \"  - No opinions found\\n\"\n",
        "        summary += \"\\n\\n\"\n",
        "\n",
        "        summary += \"Statistics:\\n\"\n",
        "        summary += \"\\n\".join(f\"  - {stat}\" for stat in data[\"stats\"][:5]) or \"  - No statistics found\\n\"\n",
        "        summary += \"\\n\\n\"\n",
        "\n",
        "        summary += \"Sources:\\n\"\n",
        "        summary += \"\\n\".join(f\"  - {domain} ({url})\" for domain, url in list(data[\"sources\"].items())[:5]) or \"  - No sources found\"\n",
        "\n",
        "        return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9fZZmPW0f_E0",
      "metadata": {
        "id": "9fZZmPW0f_E0"
      },
      "outputs": [],
      "source": [
        "research_agent = Agent(\n",
        "    role='Lead Deep Research Specialist',\n",
        "    goal='Conduct exhaustive, multi-source investigations to uncover novel insights and hidden patterns',\n",
        "    backstory=\"\"\"A forensic researcher with 15+ years experience in investigative technology analysis.\n",
        "    Known for digging beyond surface-level data to reveal groundbreaking connections.\n",
        "    Former head of research at MIT's Emerging Tech Lab.\"\"\",\n",
        "    verbose=True,\n",
        "    tools=[BlogResearchTool()],\n",
        "    llm=gemini_llm,\n",
        "    allow_delegation=False,\n",
        "    max_iter=5,\n",
        ")\n",
        "\n",
        "writer_agent = Agent(\n",
        "    role='Technical Content Architect',\n",
        "    goal='Transform complex research findings into authoritative, evidence-based publications',\n",
        "    backstory=\"\"\"PhD in Scientific Communication with a specialty in making deep technical research\n",
        "    accessible to executive audiences. Developed the 'Pyramid of Evidence' writing framework\n",
        "    used by leading tech publications.\"\"\",\n",
        "    verbose=True,\n",
        "    llm=gemini_llm,\n",
        "    allow_delegation=False,\n",
        "    max_iter=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "RRwvAFh5gGWv",
      "metadata": {
        "id": "RRwvAFh5gGWv"
      },
      "outputs": [],
      "source": [
        "research_task = Task(\n",
        "    description=\"\"\"Conduct exhaustive investigation on '{topic}' covering:\n",
        "    - Emerging patterns and underreported trends (last 6 months)\n",
        "    - Competitive landscape analysis (top 5 players benchmarking)\n",
        "    - Technical white papers and academic research (last 2 years)\n",
        "    - Regulatory developments and standardization efforts\n",
        "    - Patent analysis and R&D directions\n",
        "    - Case studies of successful/unsuccessful implementations\"\"\",\n",
        "    expected_output=\"\"\"Technical research dossier containing:\n",
        "    1. Executive Summary (100 words)\n",
        "    2. Market Thermometer: Adoption metrics + growth vectors (300 words)\n",
        "    3. Technology Deep Dive: Architectural comparisons (400 words)\n",
        "    4. Implementation Matrix: Cost/benefit analysis (200 words)\n",
        "    5. Risk Assessment: Technical debt + adoption barriers (200 words)\n",
        "    6. Future Projections: Roadmap to 2030 (300 words)\n",
        "    7. Annotated Bibliography (10+ sources)\"\"\",\n",
        "    agent=research_agent,\n",
        "    async_execution=True\n",
        ")\n",
        "\n",
        "writing_task = Task(\n",
        "    description=\"\"\"Transform the research dossier into an authoritative industry report with:\n",
        "    - Technical depth suitable for CTO/CIO audience\n",
        "    - Evidence-based arguments with proper citations [IEEE format]\n",
        "    - Comparative analysis frameworks (tables/graphs where applicable)\n",
        "    - Implementation decision trees\n",
        "    - Risk/reward evaluation matrices\n",
        "    - Vendor-neutral technology assessment\"\"\",\n",
        "    expected_output=\"\"\"Structured technical report (1000-1200 words) containing:\n",
        "    1. Catchy Heading (Phrase, h1 heading)\n",
        "    2. Disruptive Potential of {topic} (300 words, h2 heading)\n",
        "    3. Architectural Evolution Timeline (200 - 300 words, h2 heading)\n",
        "    4. Implementation Decision Framework (200 words, h2 heading)\n",
        "    5. Total Cost of Ownership Analysis (comparative table, h2 heading)\n",
        "    6. Risk Assessment Matrix (likelihood/impact grid, h2 heading)\n",
        "    7. Strategic Adoption Roadmap (phased approach, h2 heading)\n",
        "    8. Conclusion: Preparing Your Tech Stack (200 words, h3 heading)\n",
        "    9. Appendix: Methodology & Data Sources: (each source in deifferent line, h3 heading)\"\"\",\n",
        "    agent=writer_agent,\n",
        "    context=[research_task]\n",
        "\n",
        "    # if you wish to save the .md file everytime,\n",
        "    # output_file=\"industry_report.md\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PVrK_gfWgH2o",
      "metadata": {
        "id": "PVrK_gfWgH2o"
      },
      "outputs": [],
      "source": [
        "blog_crew = Crew(\n",
        "    agents=[research_agent, writer_agent],\n",
        "    tasks=[research_task, writing_task],\n",
        "    process=Process.sequential,\n",
        "    llm=gemini_llm,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "blog_topic = \"Pollution Suppresion Techniques\"\n",
        "result = blog_crew.kickoff(inputs={'topic': blog_topic})\n",
        "\n",
        "print(\"\\n\\n-------------------- FINAL BLOG POST -------------------\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fUgzh0pti_n7",
      "metadata": {
        "id": "fUgzh0pti_n7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "display(Markdown(result.raw))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "with open('research_results.md', 'w') as f:\n",
        "    f.write(result.raw)\n",
        "\n",
        "files.download('research_results.md')"
      ],
      "metadata": {
        "id": "Fbo223kGsadc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b878a617-ad90-412b-ce4f-1004952f2177"
      },
      "id": "Fbo223kGsadc",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a4fba00a-c2e5-4d98-9b31-aac70a6beb6c\", \"research_results.md\", 12989)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}