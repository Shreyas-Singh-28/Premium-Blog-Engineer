{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N2R1_Suff4rx",
      "metadata": {
        "id": "N2R1_Suff4rx"
      },
      "outputs": [],
      "source": [
        "!pip install \"crewai[tools]\" beautifulsoup4 requests transformers ddgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X3g2jkRmzSGE",
      "metadata": {
        "id": "X3g2jkRmzSGE"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W9-wOqaa8IVJ",
      "metadata": {
        "id": "W9-wOqaa8IVJ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kYdFHbfzf7Qm",
      "metadata": {
        "id": "kYdFHbfzf7Qm"
      },
      "outputs": [],
      "source": [
        "from crewai import Agent, Task, Crew, Process, LLM\n",
        "from google.colab import userdata\n",
        "\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "gemini_llm = LLM(\n",
        "    model='gemini/gemini-2.5-pro',\n",
        "    api_key=gemini_api_key,\n",
        "    temperature=0.8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-GjOHvL2veft",
      "metadata": {
        "id": "-GjOHvL2veft"
      },
      "outputs": [],
      "source": [
        "class BlogResearchTool(BaseTool):\n",
        "    name: str = \"Blog Research Assistant\"\n",
        "    description: str = \"Retrieves up-to-date information and data from the web for blog content creation\"\n",
        "\n",
        "    def _run(self, topic: str, max_results: int = 5) -> str:\n",
        "        results = []\n",
        "        with DDGS() as ddgs:\n",
        "            results = [{\n",
        "                \"title\": r.get(\"title\", \"\"),\n",
        "                \"url\": r.get(\"url\", \"\"),\n",
        "                \"snippet\": r.get(\"body\", \"\"),\n",
        "                \"source\": urlparse(r.get(\"url\", \"\")).netloc.replace(\"www.\", \"\") or \"unknown\"\n",
        "            } for r in ddgs.text(topic, max_results=max_results)]\n",
        "\n",
        "        data = {\n",
        "            \"trends\": [],\n",
        "            \"stats\": [],\n",
        "            \"devs\": [],\n",
        "            \"opinions\": [],\n",
        "            \"sources\": {r[\"source\"]: r[\"url\"] for r in results}\n",
        "        }\n",
        "\n",
        "        patterns = {\n",
        "            \"stats\": r'\\b\\d+[\\d,%\\.]+\\s*(?:percent|%|million|billion|years?|months?|\\$)',\n",
        "            \"trends\": r'\\b(?:growth|decline|increase|decrease|trend|adoption|emerging)\\b.*?[\\.\\!\\?]',\n",
        "            \"devs\": r'\\b(?:announce|launch|release|introduce|new|update|version)\\b.*?[\\.\\!\\?]',\n",
        "            \"opinions\": r'[\"“](.*?)[”\"]\\s*—\\s*\\w+\\s*\\w+'\n",
        "        }\n",
        "\n",
        "        for r in results:\n",
        "\n",
        "            for key, pattern in patterns.items():\n",
        "                matches = re.findall(pattern, r[\"snippet\"], re.IGNORECASE)\n",
        "                for match in matches:\n",
        "                    if match not in data[key]:\n",
        "                        data[key].append(match)\n",
        "\n",
        "            if r[\"source\"] in [\"techcrunch.com\", \"wired.com\", \"mit.edu\"]:\n",
        "                try:\n",
        "                    response = requests.get(r[\"url\"], timeout=10)\n",
        "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                    for tag in [\"script\", \"style\", \"header\", \"footer\", \"nav\"]:\n",
        "                        for element in soup.find_all(tag):\n",
        "                            element.decompose()\n",
        "\n",
        "                    main_content = soup.find('main') or soup.find('article') or soup.body\n",
        "                    text = main_content.get_text(separator=' ', strip=True) if main_content else \"\"\n",
        "                    for key, pattern in patterns.items():\n",
        "                        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "                        for match in matches:\n",
        "                            if match not in data[key]:\n",
        "                                data[key].append(match)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {r['url']}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        summary = f\"Research Summary for '{topic}':\\n\\n\"\n",
        "\n",
        "        summary += \"Key Trends:\\n\"\n",
        "        if data[\"trends\"]:\n",
        "            for trend in data[\"trends\"][:5]:\n",
        "                summary += f\"  - {trend}\\n\"\n",
        "        else:\n",
        "            summary += \"  - No trends found\\n\"\n",
        "        summary += \"\\n\"\n",
        "\n",
        "        summary += \"Latest Developments:\\n\"\n",
        "        if data[\"devs\"]:\n",
        "            for dev in data[\"devs\"][:5]:\n",
        "                summary += f\"  - {dev}\\n\"\n",
        "        else:\n",
        "            summary += \"  - No developments found\\n\"\n",
        "        summary += \"\\n\"\n",
        "\n",
        "        summary += \"Expert Opinions:\\n\"\n",
        "        if data[\"opinions\"]:\n",
        "            for opinion in data[\"opinions\"][:5]:\n",
        "                summary += f\"  - {opinion}\\n\"\n",
        "        else:\n",
        "            summary += \"  - No opinions found\\n\"\n",
        "        summary += \"\\n\"\n",
        "\n",
        "        summary += \"Statistics:\\n\"\n",
        "        if data[\"stats\"]:\n",
        "            for stat in data[\"stats\"][:5]:\n",
        "                summary += f\"  - {stat}\\n\"\n",
        "        else:\n",
        "            summary += \"  - No statistics found\\n\"\n",
        "        summary += \"\\n\"\n",
        "\n",
        "        summary += \"Sources:\\n\"\n",
        "        if data[\"sources\"]:\n",
        "            for domain, url in list(data[\"sources\"].items())[:5]:\n",
        "                summary += f\"  - {domain} ({url})\\n\"\n",
        "        else:\n",
        "            summary += \"  - No sources found\\n\"\n",
        "\n",
        "        return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fZZmPW0f_E0",
      "metadata": {
        "id": "9fZZmPW0f_E0"
      },
      "outputs": [],
      "source": [
        "research_agent = Agent(\n",
        "    role='Lead Deep Research Specialist',\n",
        "    goal='Conduct exhaustive, multi-source investigations to uncover novel insights and hidden patterns',\n",
        "    backstory=\"\"\"A forensic researcher with 15+ years experience in investigative technology analysis.\n",
        "    Known for digging beyond surface-level data to reveal groundbreaking connections.\n",
        "    Former head of research at MIT's Emerging Tech Lab.\"\"\",\n",
        "    verbose=True,\n",
        "    tools=[BlogResearchTool()],\n",
        "    llm=gemini_llm,\n",
        "    allow_delegation=False,\n",
        "    max_iter=5,\n",
        ")\n",
        "\n",
        "writer_agent = Agent(\n",
        "    role='Technical Content Architect',\n",
        "    goal='Transform complex research findings into authoritative, evidence-based publications',\n",
        "    backstory=\"\"\"PhD in Scientific Communication with a specialty in making deep technical research\n",
        "    accessible to executive audiences. Developed the 'Pyramid of Evidence' writing framework\n",
        "    used by leading tech publications.\"\"\",\n",
        "    verbose=True,\n",
        "    llm=gemini_llm,\n",
        "    allow_delegation=False,\n",
        "    max_iter=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RRwvAFh5gGWv",
      "metadata": {
        "id": "RRwvAFh5gGWv"
      },
      "outputs": [],
      "source": [
        "research_task = Task(\n",
        "    description=\"\"\"Conduct exhaustive investigation on '{topic}' covering:\n",
        "    - Emerging patterns and underreported trends (last 6 months)\n",
        "    - Competitive landscape analysis (top 5 players benchmarking)\n",
        "    - Technical white papers and academic research (last 2 years)\n",
        "    - Regulatory developments and standardization efforts\n",
        "    - Patent analysis and R&D directions\n",
        "    - Case studies of successful/unsuccessful implementations\"\"\",\n",
        "    expected_output=\"\"\"Technical research dossier containing:\n",
        "    1. Executive Summary (100 words)\n",
        "    2. Market Thermometer: Adoption metrics + growth vectors (300 words)\n",
        "    3. Technology Deep Dive: Architectural comparisons (400 words)\n",
        "    4. Implementation Matrix: Cost/benefit analysis (200 words)\n",
        "    5. Risk Assessment: Technical debt + adoption barriers (200 words)\n",
        "    6. Future Projections: Roadmap to 2030 (300 words)\n",
        "    7. Annotated Bibliography (10+ sources)\"\"\",\n",
        "    agent=research_agent,\n",
        "    async_execution=True\n",
        ")\n",
        "\n",
        "writing_task = Task(\n",
        "    description=\"\"\"Transform the research dossier into an authoritative industry report with:\n",
        "    - Technical depth suitable for CTO/CIO audience\n",
        "    - Evidence-based arguments with proper citations [IEEE format]\n",
        "    - Comparative analysis frameworks (tables/graphs where applicable)\n",
        "    - Implementation decision trees\n",
        "    - Risk/reward evaluation matrices\n",
        "    - Vendor-neutral technology assessment\"\"\",\n",
        "    expected_output=\"\"\"Structured technical report (1000-1200 words) containing:\n",
        "    1. Catchy Heading (Phrase, h1 heading)\n",
        "    2. Disruptive Potential of {topic} (300 words, h2 heading)\n",
        "    3. Architectural Evolution Timeline (200 - 300 words, h2 heading)\n",
        "    4. Implementation Decision Framework (200 words, h2 heading)\n",
        "    5. Total Cost of Ownership Analysis (comparative table, h2 heading)\n",
        "    6. Risk Assessment Matrix (likelihood/impact grid, h2 heading)\n",
        "    7. Strategic Adoption Roadmap (phased approach, h2 heading)\n",
        "    8. Conclusion: Preparing Your Tech Stack (200 words, h3 heading)\n",
        "    9. Appendix: Methodology & Data Sources: (each source in deifferent line, h3 heading)\"\"\",\n",
        "    agent=writer_agent,\n",
        "    context=[research_task],\n",
        "    # output_file=\"industry_report.md\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PVrK_gfWgH2o",
      "metadata": {
        "id": "PVrK_gfWgH2o"
      },
      "outputs": [],
      "source": [
        "blog_crew = Crew(\n",
        "    agents=[research_agent, writer_agent],\n",
        "    tasks=[research_task, writing_task],\n",
        "    process=Process.sequential,\n",
        "    llm=gemini_llm,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "blog_topic = \"Computer Science\"\n",
        "result = blog_crew.kickoff(inputs={'topic': blog_topic})\n",
        "\n",
        "print(\"\\n\\n-------------------- FINAL BLOG POST -------------------\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fUgzh0pti_n7",
      "metadata": {
        "id": "fUgzh0pti_n7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "display(Markdown(result.raw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fbo223kGsadc",
      "metadata": {
        "id": "Fbo223kGsadc"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "with open('research_results.md', 'w') as f:\n",
        "    f.write(result.raw)\n",
        "\n",
        "files.download('research_results.md')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
